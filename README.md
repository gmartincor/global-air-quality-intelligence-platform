# ğŸŒ Global Air Quality Intelligence Platform

Production-grade data engineering platform demonstrating Lambda Architecture with real-time and batch processing for air quality monitoring.


## ğŸ¯ Project Overview

This platform showcases enterprise-level data engineering practices through:

- **Lambda Architecture** - Speed layer (Flink/Kinesis) + Batch layer (Spark/Airflow) + Serving layer
- **Medallion Architecture** - Bronze (raw) â†’ Silver (cleaned) â†’ Gold (business) data flow
- **Infrastructure as Code** - Terraform modules for reproducible infrastructure
- **LocalStack** - Zero-cost AWS simulation (S3, Kinesis, DynamoDB)
- **Docker-First Development** - Consistent environment across development and production
- **Clean Code** - SOLID, DRY, KISS, YAGNI principles applied throughout

## ğŸ—ï¸ Architecture

```
Data Generation â†’ Speed Layer (Kinesis/Flink) â†’ DynamoDB (24h cache)
                â†“
                Batch Layer (Airflow/Spark) â†’ Delta Lake â†’ dbt â†’ PostgreSQL
                â†“
                Serving Layer (Unified API + Dashboards)
```

## ğŸš€ Quick Start

### Prerequisites

- Docker Desktop 4.20+
- Make

### Installation

```bash
git clone <repo-url>
cd global-air-quality

make dev-build
make up
make terraform-init
make terraform-apply
```

For detailed setup instructions, see [GETTING_STARTED.md](GETTING_STARTED.md).

### Access UIs

- **Airflow**: http://localhost:8080 (admin/admin)
- **Spark**: http://localhost:8081
- **LocalStack**: http://localhost:4566

### Development Workflow

```bash
make dev-shell
make format
make lint
make test
```

## ğŸ“ Project Structure

```
global-air-quality/
â”œâ”€â”€ infrastructure/
â”‚   â”œâ”€â”€ terraform/
â”‚   â”‚   â””â”€â”€ modules/
â”‚   â””â”€â”€ docker/
â”‚       â”œâ”€â”€ docker-compose.yml
â”‚       â””â”€â”€ Dockerfile
â”œâ”€â”€ src/
â”‚   â””â”€â”€ common/               # Shared utilities (DRY principle)
â”‚       â”œâ”€â”€ config.py         # Type-safe configuration with Pydantic
â”‚       â”œâ”€â”€ logger.py         # Centralized logging
â”‚       â”œâ”€â”€ exceptions.py     # Custom exception hierarchy
â”‚       â”œâ”€â”€ aws/
â”‚       â”‚   â””â”€â”€ client_factory.py  # AWS client factory with error handling
â”‚       â””â”€â”€ utils/
â”‚           â”œâ”€â”€ validators.py # Input validation functions
â”‚           â””â”€â”€ retry.py      # Retry decorator with backoff
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ unit/                 # Unit tests with >90% coverage
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml          # Environment configuration
â”œâ”€â”€ .pre-commit-config.yaml  # Code quality hooks
â”œâ”€â”€ pyproject.toml           # Poetry dependencies
â”œâ”€â”€ Makefile                 # Automation commands
â””â”€â”€ README.md
```

## ğŸ› ï¸ Technology Stack

**Infrastructure**: Terraform, LocalStack, Docker Compose  
**Processing**: Apache Spark, Apache Flink, dbt  
**Orchestration**: Apache Airflow  
**Storage**: S3 (LocalStack), Delta Lake, PostgreSQL, DynamoDB  
**Quality**: Great Expectations, pytest  
**Development**: Docker, Poetry, Make



## ğŸ› ï¸ Development Commands

```bash
make dev-build        
make up               
make dev-shell        

make format           
make lint             
make type-check       
make test             
make test-cov         

make logs             
make ps               
make restart          

make terraform-init   
make terraform-plan   
make terraform-apply  
make terraform-destroy

make clean            
make clean-all        
```
